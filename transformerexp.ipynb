{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformerexp.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgaO4MCL6V3v"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import spacy\n",
        "seaborn.set_context(context=\"talk\")\n",
        "##%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQHqdGxv6piO"
      },
      "source": [
        "d_model = 512\n",
        "text = \"I am a student\"\n",
        "spacy_en = spacy.load('en_core_web_sm') # using full name 'en' -> 'en_core_web_sm'\n",
        "dropout = 0.1\n",
        "\n",
        "def tokenize(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "vocab = tokenize(text)\n",
        "vocab_len = len(vocab)\n",
        "print(vocab, len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIhbhvyPLkk6"
      },
      "source": [
        "# Word embedding test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvNUpF2v6sQF"
      },
      "source": [
        "class Embeddings(nn.Module) :\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)  # (seq_len, d_model) embedding\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model) # multiply sqrt(d_model) to embeded result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFUNr9Mf6wZ-"
      },
      "source": [
        "x = nn.Embedding(len(vocab), d_model)\n",
        "print(x.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-soQ2uXnI6kI"
      },
      "source": [
        "def forward(d_model, vocab_len):\n",
        "    lut = nn.Embedding(vocab_len, d_model)\n",
        "    return lut\n",
        "\n",
        "test = forward(d_model, vocab_len)\n",
        "print(test.weight)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9atWK4DJRsEh"
      },
      "source": [
        "tmp = Embeddings(d_model, vocab_len)\n",
        "tmp"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}